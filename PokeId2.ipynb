{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PokeId2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samuelKratsas/PokeType-Identifier/blob/master/PokeId2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIHKyTrFZ5-o",
        "colab_type": "text"
      },
      "source": [
        "##PokeId Final Model and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEq1oonCdBSw",
        "colab_type": "code",
        "outputId": "3481ef5d-a0bb-4622-a17b-921cb245309c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghEB_dl7W_Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.layers import Dense,Conv2D,Activation,MaxPooling2D,Flatten,Dropout\n",
        "from tensorflow.python.keras import Sequential\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psjsm6HIZl-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Conv2D(32,(3,3),input_shape=(120,120,3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size =(2,2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Conv2D(64, (3, 3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Conv2D(128, (3, 3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(512))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Dense(18))\n",
        "classifier.add(Activation('sigmoid'))\n",
        "\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHXkToVFa4kI",
        "colab_type": "code",
        "outputId": "062b9839-0a73-49d2-c383-7e4ca5bfe121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "## Summary of the model\n",
        "classifier.summary()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 118, 118, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 118, 118, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 59, 59, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 59, 59, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 57, 57, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 57, 57, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 26, 26, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 21632)             0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 512)               11076096  \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 18)                9234      \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 18)                0         \n",
            "=================================================================\n",
            "Total params: 11,178,578\n",
            "Trainable params: 11,178,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhluT1cib6X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ftQI3q61_Yc",
        "colab_type": "text"
      },
      "source": [
        "##ORIGINAL TEST CASE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWl9Lbe-b-pZ",
        "colab_type": "code",
        "outputId": "2e60eb39-1a96-428d-8274-e88687cc03ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator()\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/output/train',\n",
        "                                                target_size=(120,120),\n",
        "                                                batch_size= 50,\n",
        "                                                class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/output/test',\n",
        "                                           target_size = (120,120),\n",
        "                                           batch_size = 50,\n",
        "                                           class_mode ='categorical')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 639 images belonging to 18 classes.\n",
            "Found 98 images belonging to 18 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB8m-CwccGCz",
        "colab_type": "code",
        "outputId": "6252f096-8433-4601-92c1-1e2c8d9fc75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "#steps per epoch = trainingTotal/batch size\n",
        "\n",
        "classifier.fit_generator(training_set,\n",
        "                        steps_per_epoch = (639/50),\n",
        "                        epochs = 30,\n",
        "                        validation_data = test_set,\n",
        "                        callbacks=[EarlyStopping(patience=2)],\n",
        "                        validation_steps = (98/50))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "13/12 [==============================] - 9s 731ms/step - loss: 8.9431 - accuracy: 0.0829 - val_loss: 8.7315 - val_accuracy: 0.0816\n",
            "Epoch 2/30\n",
            "13/12 [==============================] - 10s 733ms/step - loss: 8.9176 - accuracy: 0.0876 - val_loss: 8.5576 - val_accuracy: 0.1020\n",
            "Epoch 3/30\n",
            "13/12 [==============================] - 9s 729ms/step - loss: 9.4827 - accuracy: 0.0876 - val_loss: 9.7848 - val_accuracy: 0.0816\n",
            "Epoch 4/30\n",
            "13/12 [==============================] - 9s 726ms/step - loss: 9.4793 - accuracy: 0.0876 - val_loss: 10.6039 - val_accuracy: 0.0612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff3c21b99e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABfnjQ8UgIFz",
        "colab_type": "code",
        "outputId": "99adac74-a6d4-4799-c376-d4a48fc1d4ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classifier.save('poke_cnn_model.AllTypes')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: poke_cnn_model.AllTypes/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqJX7yjO2VO0",
        "colab_type": "text"
      },
      "source": [
        "##Grass Bug VS Ground Steel Rock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2bLnsm_M6BKm",
        "colab": {}
      },
      "source": [
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Conv2D(32,(3,3),input_shape=(120,120,3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size =(2,2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Conv2D(64, (3, 3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Conv2D(128, (3, 3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(512))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Dense(2))\n",
        "classifier.add(Activation('sigmoid'))\n",
        "\n",
        "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "592beff3-227c-488f-b158-95e8427fb9ea",
        "id": "aVAG8sF648ya",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator()\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/gbRGS/train',\n",
        "                                                target_size=(120,120),\n",
        "                                                batch_size= 2,\n",
        "                                                class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/gbRGS/test',\n",
        "                                           target_size = (120,120),\n",
        "                                           batch_size = 2,\n",
        "                                           class_mode ='categorical')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 176 images belonging to 2 classes.\n",
            "Found 28 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f994b566-9eba-4200-8f95-8ef5c39ef6f1",
        "id": "ubXPn-f95C9Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "#steps per epoch = trainingTotal/batch size\n",
        "\n",
        "classifier.fit_generator(training_set,\n",
        "                        steps_per_epoch = (176/2),\n",
        "                        epochs = 30,\n",
        "                        validation_data = test_set,\n",
        "                        callbacks=[EarlyStopping(patience=2)],\n",
        "                        validation_steps = (28/2))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "88/88 [==============================] - 10s 118ms/step - loss: 1.0526 - accuracy: 0.5739 - val_loss: 0.6931 - val_accuracy: 0.5357\n",
            "Epoch 2/30\n",
            "88/88 [==============================] - 11s 119ms/step - loss: 0.6931 - accuracy: 0.5795 - val_loss: 0.6931 - val_accuracy: 0.5357\n",
            "Epoch 3/30\n",
            "88/88 [==============================] - 10s 118ms/step - loss: 0.6931 - accuracy: 0.5795 - val_loss: 0.6931 - val_accuracy: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff3c3228320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4d133682-d1a1-4712-b18f-ee08ac2720c9",
        "id": "DVR__zXL5Fgg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classifier.save('poke_cnn_model.gbRGS')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: poke_cnn_model.gbRGS/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IMpHqMj36Ury"
      },
      "source": [
        "##Water VS Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OQkxZYnS6Urz",
        "colab": {}
      },
      "source": [
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Conv2D(32,(3,3),input_shape=(120,120,3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size =(2,2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Conv2D(64, (3, 3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Conv2D(128, (3, 3)))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Dropout(0.25))\n",
        "\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(512))\n",
        "classifier.add(Activation('relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(Dense(1))\n",
        "classifier.add(Activation('sigmoid'))\n",
        "\n",
        "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9549d0e3-b67d-4dc6-ada7-4a58299c005f",
        "id": "qN59JNd06Ur5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator()\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/waterNormal/train',\n",
        "                                                target_size=(120,120),\n",
        "                                                batch_size= 2,\n",
        "                                                class_mode='binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/waterNormal/test',\n",
        "                                           target_size = (120,120),\n",
        "                                           batch_size = 2,\n",
        "                                           class_mode ='binary')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 132 images belonging to 2 classes.\n",
            "Found 19 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "67ca88b2-e154-429f-ca9a-f5460c3a5356",
        "id": "G8WhpenE6Ur-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "#steps per epoch = trainingTotal/batch size\n",
        "\n",
        "classifier.fit_generator(training_set,\n",
        "                        steps_per_epoch = (132/2),\n",
        "                        epochs = 30,\n",
        "                        validation_data = test_set,\n",
        "                        callbacks=[EarlyStopping(patience=2)],\n",
        "                        validation_steps = (19/2))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " 2/66 [..............................] - ETA: 3s - loss: 17.3531 - accuracy: 0.5000   "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "66/66 [==============================] - 7s 112ms/step - loss: 35.3708 - accuracy: 0.5303 - val_loss: 0.6807 - val_accuracy: 0.5789\n",
            "Epoch 2/30\n",
            "66/66 [==============================] - 7s 108ms/step - loss: 0.5253 - accuracy: 0.7348 - val_loss: 0.8270 - val_accuracy: 0.6842\n",
            "Epoch 3/30\n",
            "66/66 [==============================] - 7s 109ms/step - loss: 0.4566 - accuracy: 0.7727 - val_loss: 0.8093 - val_accuracy: 0.6842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff3c27eacf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c8b2ce3f-0a29-4870-fa09-db0da17b1ced",
        "id": "GTKwRlUk6UsE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classifier.save('poke_cnn_model.waterNormal')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: poke_cnn_model.waterNormal/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4qy5ca85GS7",
        "colab_type": "text"
      },
      "source": [
        "##Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0JP8T6V_4uS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import load_model \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCFoGZLG_o7L",
        "colab_type": "text"
      },
      "source": [
        "###All Types Results Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5_DlJlUgWxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = load_model('poke_cnn_model.AllTypes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EuAp1DbA7Lt",
        "colab_type": "code",
        "outputId": "20347795-92f3-4413-c2ca-aa917c644360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/output/train',\n",
        "                                                target_size=(120,120),\n",
        "                                                batch_size= 2,\n",
        "                                                class_mode='categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_set = test_datagen.flow_from_directory('/content/drive/My Drive/output/val',\n",
        "                                           target_size = (120,120),\n",
        "                                           batch_size = 50,\n",
        "                                           class_mode ='categorical',\n",
        "                                           shuffle=False)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 639 images belonging to 18 classes.\n",
            "Found 72 images belonging to 18 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvtsAv1zgZmy",
        "colab_type": "code",
        "outputId": "538b9d7c-869d-4b16-cb4a-ce35207083c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_set.reset()\n",
        "\n",
        "predict = classifier.predict_generator(val_set, verbose=1, steps = (72/2))\n",
        "preds_cls_idx = predict.argmax(axis=-1)\n",
        "idx_to_cls = {v: k for k, v in training_set.class_indices.items()}\n",
        "preds_cls = np.vectorize(idx_to_cls.get)(preds_cls_idx)\n",
        "filenames_to_cls = list(zip(val_set.filenames, preds_cls))\n",
        "correct=0\n",
        "wrong=0\n",
        "for i in filenames_to_cls:\n",
        "  actual,sep,tail = i[0].partition('/')\n",
        "  print(i[0])\n",
        "  prediction = i[1]\n",
        "  print(prediction)\n",
        "  print('\\n')\n",
        "  if actual == prediction:\n",
        "    correct+=1\n",
        "  else:\n",
        "    wrong+=1\n",
        "\n",
        "print('Correct:',correct)\n",
        "print('Wrong:',wrong)\n",
        "print(correct/wrong)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 6s 154ms/step\n",
            "class1/blissey.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/deerling.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/glameow.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/porygon2.png\n",
            "class4\n",
            "\n",
            "\n",
            "class1/regigigas.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/smeargle.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/whismur.png\n",
            "class1\n",
            "\n",
            "\n",
            "class10/golem.png\n",
            "class1\n",
            "\n",
            "\n",
            "class10/lycanroc-midday.png\n",
            "class1\n",
            "\n",
            "\n",
            "class10/tyrantrum.png\n",
            "class1\n",
            "\n",
            "\n",
            "class11/magneton.png\n",
            "class1\n",
            "\n",
            "\n",
            "class11/metang.png\n",
            "class1\n",
            "\n",
            "\n",
            "class12/piloswine.png\n",
            "class1\n",
            "\n",
            "\n",
            "class12/stunfisk.png\n",
            "class1\n",
            "\n",
            "\n",
            "class13/farfetchd.png\n",
            "class1\n",
            "\n",
            "\n",
            "class13/hoothoot.png\n",
            "class1\n",
            "\n",
            "\n",
            "class13/moltres.png\n",
            "class1\n",
            "\n",
            "\n",
            "class13/swoobat.png\n",
            "class1\n",
            "\n",
            "\n",
            "class13/togekiss.png\n",
            "class1\n",
            "\n",
            "\n",
            "class13/tranquill.png\n",
            "class1\n",
            "\n",
            "\n",
            "class14/emolga.png\n",
            "class1\n",
            "\n",
            "\n",
            "class14/luxray.png\n",
            "class1\n",
            "\n",
            "\n",
            "class14/zebstrika.png\n",
            "class1\n",
            "\n",
            "\n",
            "class15/dewgong.png\n",
            "class1\n",
            "\n",
            "\n",
            "class15/snorunt.png\n",
            "class4\n",
            "\n",
            "\n",
            "class16/giratina-altered.png\n",
            "class1\n",
            "\n",
            "\n",
            "class16/kyurem.png\n",
            "class1\n",
            "\n",
            "\n",
            "class16/sliggoo.png\n",
            "class1\n",
            "\n",
            "\n",
            "class17/marshadow.png\n",
            "class1\n",
            "\n",
            "\n",
            "class17/shuppet.png\n",
            "class7\n",
            "\n",
            "\n",
            "class18/gloom.png\n",
            "class1\n",
            "\n",
            "\n",
            "class18/nidorina.png\n",
            "class1\n",
            "\n",
            "\n",
            "class18/poipole.png\n",
            "class1\n",
            "\n",
            "\n",
            "class18/weepinbell.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/heatran.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/litleo.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/magcargo.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/typhlosion.png\n",
            "class1\n",
            "\n",
            "\n",
            "class3/chespin.png\n",
            "class1\n",
            "\n",
            "\n",
            "class3/grovyle.png\n",
            "class1\n",
            "\n",
            "\n",
            "class3/serperior.png\n",
            "class1\n",
            "\n",
            "\n",
            "class3/shiftry.png\n",
            "class1\n",
            "\n",
            "\n",
            "class3/sunflora.png\n",
            "class1\n",
            "\n",
            "\n",
            "class3/venusaur.png\n",
            "class1\n",
            "\n",
            "\n",
            "class4/carvanha.png\n",
            "class4\n",
            "\n",
            "\n",
            "class4/corsola.png\n",
            "class1\n",
            "\n",
            "\n",
            "class4/goldeen.png\n",
            "class4\n",
            "\n",
            "\n",
            "class4/lapras.png\n",
            "class1\n",
            "\n",
            "\n",
            "class4/mudkip.png\n",
            "class1\n",
            "\n",
            "\n",
            "class4/octillery.png\n",
            "class1\n",
            "\n",
            "\n",
            "class4/oshawott.png\n",
            "class4\n",
            "\n",
            "\n",
            "class4/prinplup.png\n",
            "class1\n",
            "\n",
            "\n",
            "class4/tentacruel.png\n",
            "class1\n",
            "\n",
            "\n",
            "class5/caterpie.png\n",
            "class7\n",
            "\n",
            "\n",
            "class5/heracross.png\n",
            "class1\n",
            "\n",
            "\n",
            "class5/karrablast.png\n",
            "class4\n",
            "\n",
            "\n",
            "class5/ribombee.png\n",
            "class1\n",
            "\n",
            "\n",
            "class5/shelmet.png\n",
            "class4\n",
            "\n",
            "\n",
            "class5/spinarak.png\n",
            "class7\n",
            "\n",
            "\n",
            "class6/jigglypuff.png\n",
            "class1\n",
            "\n",
            "\n",
            "class6/snubbull.png\n",
            "class1\n",
            "\n",
            "\n",
            "class6/tapu-lele.png\n",
            "class1\n",
            "\n",
            "\n",
            "class7/malamar.png\n",
            "class1\n",
            "\n",
            "\n",
            "class7/sandile.png\n",
            "class4\n",
            "\n",
            "\n",
            "class8/cresselia.png\n",
            "class1\n",
            "\n",
            "\n",
            "class8/gothitelle.png\n",
            "class1\n",
            "\n",
            "\n",
            "class8/hoopa-confined.png\n",
            "class1\n",
            "\n",
            "\n",
            "class8/musharna.png\n",
            "class1\n",
            "\n",
            "\n",
            "class8/slowpoke.png\n",
            "class1\n",
            "\n",
            "\n",
            "class9/conkeldurr.png\n",
            "class1\n",
            "\n",
            "\n",
            "class9/hitmonchan.png\n",
            "class1\n",
            "\n",
            "\n",
            "class9/primeape.png\n",
            "class1\n",
            "\n",
            "\n",
            "Correct: 9\n",
            "Wrong: 63\n",
            "0.14285714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9kgAsSfOwfXC"
      },
      "source": [
        "###Grass Bug VS Ground Steel Rock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "acgCDnohwfXJ",
        "colab": {}
      },
      "source": [
        "classifier = load_model('poke_cnn_model.gbRGS')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d4602c45-07d6-45fd-982e-b35b5e5d79d1",
        "id": "ifofl480wfXO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/gbRGS/train',\n",
        "                                                target_size=(120,120),\n",
        "                                                batch_size= 2,\n",
        "                                                class_mode='binary',\n",
        "                                                shuffle=False)\n",
        "\n",
        "val_set = test_datagen.flow_from_directory('/content/drive/My Drive/gbRGS/val',\n",
        "                                           target_size = (120,120),\n",
        "                                           batch_size = 2,\n",
        "                                           class_mode ='binary',\n",
        "                                           shuffle=False)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 176 images belonging to 2 classes.\n",
            "Found 19 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e0ad17b2-ec16-4111-985e-49fddb1133bd",
        "id": "EprjjZU-wfXR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_set.reset()\n",
        "\n",
        "predict = classifier.predict_generator(val_set, verbose=1, steps = (19/2))\n",
        "preds_cls_idx = predict.argmax(axis=-1)\n",
        "idx_to_cls = {v: k for k, v in training_set.class_indices.items()}\n",
        "preds_cls = np.vectorize(idx_to_cls.get)(preds_cls_idx)\n",
        "filenames_to_cls = list(zip(val_set.filenames, preds_cls))\n",
        "correct=0\n",
        "wrong=0\n",
        "for i in filenames_to_cls:\n",
        "  actual,sep,tail = i[0].partition('/')\n",
        "  print(i[0])\n",
        "  prediction = i[1]\n",
        "  print(prediction)\n",
        "  print('\\n')\n",
        "  if actual == prediction:\n",
        "    correct+=1\n",
        "  else:\n",
        "    wrong+=1\n",
        "\n",
        "print('Correct:',correct)\n",
        "print('Wrong:',wrong)\n",
        "print(correct/wrong)\n"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/9 [===============================] - 0s 14ms/step\n",
            "class1/caterpie.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/chespin.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/grovyle.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/heracross.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/karrablast.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/ribombee.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/serperior.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/shelmet.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/shiftry.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/spinarak.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/sunflora.png\n",
            "class2\n",
            "\n",
            "\n",
            "class1/venusaur.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/golem.png\n",
            "class2\n",
            "\n",
            "\n",
            "class2/lycanroc-midday.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/magneton.png\n",
            "class2\n",
            "\n",
            "\n",
            "class2/malamar.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/metang.png\n",
            "class2\n",
            "\n",
            "\n",
            "class2/sandile.png\n",
            "class2\n",
            "\n",
            "\n",
            "class2/tyrantrum.png\n",
            "class1\n",
            "\n",
            "\n",
            "Correct: 7\n",
            "Wrong: 12\n",
            "0.5833333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LXVnah-gwghB"
      },
      "source": [
        "###Water VS Normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gPTb0xBPwghE",
        "colab": {}
      },
      "source": [
        "classifier = load_model('poke_cnn_model.waterNormal')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "30c55cf9-952a-43cc-f80f-abd6cf184bdb",
        "id": "hiwnvkUqwghH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/waterNormal/train',\n",
        "                                                target_size=(120,120),\n",
        "                                                batch_size= 2,\n",
        "                                                class_mode='binary')\n",
        "\n",
        "val_set = test_datagen.flow_from_directory('/content/drive/My Drive/waterNormal/val',\n",
        "                                           target_size = (120,120),\n",
        "                                           batch_size = 2,\n",
        "                                           class_mode ='binary',\n",
        "                                           shuffle=False)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 132 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9d246ab0-f33d-421a-c69d-34738de8022d",
        "id": "jUyqAdh4wghJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_set.reset()\n",
        "\n",
        "predict = classifier.predict_generator(val_set, verbose=1, steps = (16/2))\n",
        "preds_cls_idx = predict.argmax(axis=-1)\n",
        "idx_to_cls = {v: k for k, v in training_set.class_indices.items()}\n",
        "preds_cls = np.vectorize(idx_to_cls.get)(preds_cls_idx)\n",
        "filenames_to_cls = list(zip(val_set.filenames, preds_cls))\n",
        "correct=0\n",
        "wrong=0\n",
        "for i in filenames_to_cls:\n",
        "  actual,sep,tail = i[0].partition('/')\n",
        "  print(i[0])\n",
        "  prediction = i[1]\n",
        "  print(prediction)\n",
        "  print('\\n')\n",
        "  if actual == prediction:\n",
        "    correct+=1\n",
        "  else:\n",
        "    wrong+=1\n",
        "\n",
        "print('Correct:',correct)\n",
        "print('Wrong:',wrong)\n",
        "print(correct/wrong)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 5/36 [===>..........................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "36/36 [==============================] - 1s 15ms/step\n",
            "class1/blissey.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/deerling.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/glameow.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/porygon2.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/regigigas.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/smeargle.png\n",
            "class1\n",
            "\n",
            "\n",
            "class1/whismur.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/carvanha.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/corsola.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/goldeen.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/lapras.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/mudkip.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/octillery.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/oshawott.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/prinplup.png\n",
            "class1\n",
            "\n",
            "\n",
            "class2/tentacruel.png\n",
            "class1\n",
            "\n",
            "\n",
            "Correct: 7\n",
            "Wrong: 9\n",
            "0.7777777777777778\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}